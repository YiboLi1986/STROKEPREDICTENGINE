import os
import sys
import pandas as pd
import joblib
import shutil
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from itertools import combinations
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '../../..')))

# ğŸ”¹ PyTorch ç¥ç»ç½‘ç»œæ¨¡å‹
class NeuralNetwork(nn.Module):
    """ PyTorch ç‰ˆç¥ç»ç½‘ç»œæ¨¡å‹ """
    def __init__(self, input_dim):
        super(NeuralNetwork, self).__init__()
        self.model = nn.Sequential(
            nn.Linear(input_dim, 64),
            nn.ReLU(),
            nn.Linear(64, 64),
            nn.ReLU(),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Linear(32, 1),
            nn.Sigmoid()
        )

    def forward(self, x):
        return self.model(x)

# ğŸ”¹ è®­ç»ƒç®¡ç†ç±»
class ModelTrainerWithReports:
    """ è®­ç»ƒ Random Forest, XGBoost å’Œ PyTorch ç¥ç»ç½‘ç»œ """

    def __init__(self, file_path: str, label_col: str, fixed_features: list, max_combination_size=3, base_dir="app/services/featureset_multimodels/trained_models"):
        self.dataset = pd.read_excel(file_path).dropna()
        self.label_col = label_col
        self.fixed_features = fixed_features
        self.base_dir = base_dir

        # **åˆ é™¤ `base_dir`ï¼Œç¡®ä¿æ¯æ¬¡éƒ½æ˜¯å¹²å‡€çš„è®­ç»ƒç¯å¢ƒ**
        if os.path.exists(self.base_dir):
            shutil.rmtree(self.base_dir)  # é€’å½’åˆ é™¤æ•´ä¸ªç›®å½•

        os.makedirs(self.base_dir, exist_ok=True)

        # **è‡ªåŠ¨ç”Ÿæˆ feature_sets**
        self.feature_sets = self.generate_feature_sets(max_combination_size)

    def generate_feature_sets(self, max_combination_size):
        """
        è‡ªåŠ¨ç”Ÿæˆ feature_setsï¼ˆç‰¹å¾ç»„åˆï¼‰ã€‚
        """
        all_features = list(self.dataset.columns)
        variable_features = [f for f in all_features if f not in self.fixed_features + [self.label_col]]

        feature_sets = []
        max_size = len(variable_features) if max_combination_size is None else max_combination_size
        for r in range(1, max_size + 1):
            for subset in combinations(variable_features, r):
                feature_sets.append(self.fixed_features + list(subset))

        return feature_sets

    def train_models(self):
        """ è®­ç»ƒæ‰€æœ‰æ¨¡å‹å¹¶ä¿å­˜ """
        results = {}

        for i, feature_subset in enumerate(self.feature_sets):
            feature_str = "_".join(feature_subset).replace(" ", "")
            model_dir = os.path.join(self.base_dir, feature_str)
            os.makedirs(model_dir, exist_ok=True)

            print(f"\nTraining models on feature subset {i + 1}/{len(self.feature_sets)}: {feature_subset}")

            # æ•°æ®å¤„ç†
            X = self.dataset[feature_subset].values
            y = self.dataset[self.label_col].values
            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

            # ä»…å¯¹ç¥ç»ç½‘ç»œæ ‡å‡†åŒ–
            scaler = StandardScaler()
            X_train_scaled = scaler.fit_transform(X_train)
            X_test_scaled = scaler.transform(X_test)

            # **è®­ç»ƒ Random Forest**
            rf = RandomForestClassifier(n_estimators=100, random_state=42)
            rf.fit(X_train, y_train)
            y_pred_rf = rf.predict(X_test)
            rf_report = self.evaluate_performance(y_test, y_pred_rf)
            joblib.dump(rf, os.path.join(model_dir, "random_forest.pkl"))

            # **è®­ç»ƒ XGBoost**
            xgb = XGBClassifier(eval_metric='logloss')
            xgb.fit(X_train, y_train)
            y_pred_xgb = xgb.predict(X_test)
            xgb_report = self.evaluate_performance(y_test, y_pred_xgb)
            xgb.save_model(os.path.join(model_dir, "xgboost.json"))

            # **è®­ç»ƒ PyTorch ç¥ç»ç½‘ç»œ**
            device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
            model = NeuralNetwork(input_dim=X_train.shape[1]).to(device)
            criterion = nn.BCELoss()
            optimizer = optim.Adam(model.parameters(), lr=0.001)

            # è½¬æ¢æ•°æ®ä¸º PyTorch å¼ é‡
            X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32).to(device)
            y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1).to(device)
            X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32).to(device)

            train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
            train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)

            # è®­ç»ƒå¾ªç¯
            model.train()
            for epoch in range(15):
                for X_batch, y_batch in train_loader:
                    optimizer.zero_grad()
                    y_pred = model(X_batch)
                    loss = criterion(y_pred, y_batch)
                    loss.backward()
                    optimizer.step()

            # é¢„æµ‹
            model.eval()
            with torch.no_grad():
                y_pred_nn = (model(X_test_tensor) > 0.5).cpu().numpy().astype(int)

            nn_report = self.evaluate_performance(y_test, y_pred_nn)

            # **ä¿å­˜ PyTorch æ¨¡å‹**
            torch.save(model.state_dict(), os.path.join(model_dir, "neural_network.pth"))

            # **ä¿å­˜æ€§èƒ½æŠ¥å‘Š**
            performance_report = {
                "Random Forest": rf_report,
                "XGBoost": xgb_report,
                "Neural Network": nn_report
            }
            results[feature_str] = performance_report
            self.save_performance_report(performance_report, model_dir)

            print(f"Models and performance report saved to {model_dir}")

        return results

    @staticmethod
    def evaluate_performance(y_true, y_pred):
        return {
            "Accuracy": accuracy_score(y_true, y_pred),
            "Precision": precision_score(y_true, y_pred, zero_division=0),
            "Recall": recall_score(y_true, y_pred, zero_division=0),
            "F1-Score": f1_score(y_true, y_pred, zero_division=0),
            "ROC AUC": roc_auc_score(y_true, y_pred)
        }

    @staticmethod
    def save_performance_report(report, directory):
        """
        Saves the performance report as a CSV file, including explanations.
        
        Args:
            report (dict): The performance report.
            directory (str): The directory where the report should be saved.
        """
        # è½¬æ¢æ€§èƒ½æŠ¥å‘Šä¸º DataFrame
        df = pd.DataFrame(report).T

        # ç›´æ¥æ·»åŠ ä¸€è¡Œ Explanation
        explanation_row = {
            "Accuracy": "Overall correctness of the model (correct predictions / total predictions). Higher is better. / æ¨¡å‹æ•´ä½“æ­£ç¡®ç‡ï¼ˆæ­£ç¡®é¢„æµ‹æ•° / æ€»é¢„æµ‹æ•°ï¼‰ã€‚å€¼è¶Šé«˜è¶Šå¥½ã€‚",
            "Precision": "Proportion of true positive predictions among all positive predictions (TP / (TP + FP)). Measures the modelâ€™s ability to avoid false positives. / é¢„æµ‹ä¸ºæ­£ä¾‹çš„æ ·æœ¬ä¸­ï¼ŒçœŸæ­£çš„æ­£ä¾‹æ¯”ä¾‹ï¼ˆTP / (TP + FP)ï¼‰ã€‚ç”¨äºè¡¡é‡æ¨¡å‹å‡å°‘è¯¯æŠ¥çš„èƒ½åŠ›ã€‚",
            "Recall": "Proportion of actual positive cases correctly predicted (TP / (TP + FN)). Measures the modelâ€™s ability to capture positive instances. / å®é™…æ­£ä¾‹ä¸­ï¼Œè¢«æ­£ç¡®é¢„æµ‹çš„æ¯”ä¾‹ï¼ˆTP / (TP + FN)ï¼‰ã€‚ç”¨äºè¡¡é‡æ¨¡å‹å‡å°‘æ¼æŠ¥çš„èƒ½åŠ›ã€‚",
            "F1-Score": "Harmonic mean of Precision and Recall (2 * (Precision * Recall) / (Precision + Recall)). A balanced metric when Precision and Recall are both important. / Precision å’Œ Recall çš„è°ƒå’Œå¹³å‡æ•°ï¼ˆ2 * (Precision * Recall) / (Precision + Recall)ï¼‰ã€‚ç”¨äºå¹³è¡¡ Precision å’Œ Recallã€‚",
            "ROC AUC": "Area under the ROC curve, measuring how well the model distinguishes between positive and negative classes. 1.0 means perfect distinction, 0.5 means random guessing. / ROC æ›²çº¿ä¸‹çš„é¢ç§¯ï¼Œè¡¡é‡æ¨¡å‹åŒºåˆ†æ­£è´Ÿç±»çš„èƒ½åŠ›ã€‚1.0 è¡¨ç¤ºå®Œç¾ï¼Œ0.5 è¡¨ç¤ºéšæœºçŒœæµ‹ã€‚"
        }

        # å°†è§£é‡Šä½œä¸ºæ–°çš„ä¸€è¡Œæ·»åŠ 
        explanation_df = pd.DataFrame(explanation_row, index=["Explanation"])
        df = pd.concat([df, explanation_df])

        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(directory, exist_ok=True)

        # ä¿å­˜ CSVï¼Œé¿å…ä¹±ç é—®é¢˜
        df.to_csv(os.path.join(directory, "performance_report.csv"), encoding="utf-8-sig")

        print(f"Performance report saved to {directory}/performance_report.csv")


if __name__ == "__main__":
    trainer = ModelTrainerWithReports(
        file_path="app/services/data/data_clean.xlsx",
        label_col="Output",
        fixed_features=["age", "sex"],  # å›ºå®šç‰¹å¾
        max_combination_size=1 # é™åˆ¶æœ€å¤§ç»„åˆç‰¹å¾æ•°
    )
    trainer.train_models()
